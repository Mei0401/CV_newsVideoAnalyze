{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "cv.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shot Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "all_frame_1 = []\n",
    "for i in range(22, 201):\n",
    "    name = str(i) + '.jpg'\n",
    "    if len(name) == 6:\n",
    "        name = '0' + name\n",
    "    pic = cv.imread(\"./project1/clip_1/\" + name)\n",
    "    RGB_img = cv.cvtColor(pic, cv.COLOR_BGR2RGB)\n",
    "    all_frame_1.append(RGB_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_f is a list that contains all frames of a video\n",
    "# this function returns a list that has each pair of frames' color histogram differences\n",
    "def create_hist_diff(all_f):\n",
    "    red_hists = []\n",
    "    green_hists = []\n",
    "    blue_hists = []\n",
    "    wide, height = all_f[0].shape[1], all_f[0].shape[0]\n",
    "    for img in all_f:\n",
    "        red_hists.append(cv.calcHist([img],[0],None,[256],[0,256]))\n",
    "        green_hists.append(cv.calcHist([img],[1],None,[256],[0,256]))\n",
    "        blue_hists.append(cv.calcHist([img],[2],None,[256],[0,256]))\n",
    "    result = []\n",
    "    for i in range(1,len(red_hists)):\n",
    "    #   find total number of how many pixels have different color, normallize it to 0~1\n",
    "        red_diff = np.sum(np.abs(red_hists[i] - red_hists[i-1]))\n",
    "        green_diff = np.sum(np.abs(green_hists[i] - green_hists[i-1]))\n",
    "        blue_diff = np.sum(np.abs(blue_hists[i] - blue_hists[i-1]))\n",
    "        result.append((red_diff + green_diff + blue_diff) / (3*wide*height))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precondition: shot_len > hist_diff\n",
    "# returns a list that contains the index of the boudary frames\n",
    "def find_boundary(hist_diff, shot_len = 8, shot_diff = 4, pace = 3):\n",
    "    result = []\n",
    "    prev_bound_i = 0\n",
    "    for i in range(0, len(hist_diff) - shot_len, pace):\n",
    "        # create a sliding window\n",
    "        slide_w = hist_diff[i:shot_len + i]\n",
    "        # find the frame that has largest possibility to be a boudary frame\n",
    "        suspect_frame_i = np.argmax(slide_w) + i\n",
    "        \n",
    "        # if two boudary frame are too close, skip this boudary frame\n",
    "        if suspect_frame_i - prev_bound_i < shot_len:\n",
    "            continue\n",
    "        \n",
    "        # if the boudary frame does not have largest color hist difference between his neighbour, skip it\n",
    "        is_largest_neighbour = 0\n",
    "        for neighbour in range(1,pace+1):\n",
    "            prev = suspect_frame_i - neighbour\n",
    "            after = suspect_frame_i + neighbour\n",
    "            if prev < 0 or after >= len(hist_diff):\n",
    "                break\n",
    "            if hist_diff[suspect_frame_i] <= hist_diff[prev] or hist_diff[suspect_frame_i] <= hist_diff[after]:\n",
    "                is_largest_neighbour = 1\n",
    "                break\n",
    "        if is_largest_neighbour == 1:\n",
    "            continue\n",
    "        \n",
    "        # if the color hist difference of boundary frame < shot_diff * average color hist in the window, skip\n",
    "        window_avg = np.sum(hist_diff[prev_bound_i:suspect_frame_i]) / (suspect_frame_i - prev_bound_i)\n",
    "        if window_avg * shot_diff > hist_diff[suspect_frame_i]:\n",
    "            continue\n",
    "        \n",
    "        # else, says the suspect frame is a shot boudary frame\n",
    "        prev_bound_i = suspect_frame_i\n",
    "        result.append(suspect_frame_i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameter sets for find boundar algorithm\n",
    "shot_length_list = [1,2,3,4,6,8]\n",
    "shot_diff_list = [1.5,1.8,2.5,3,4,4.5]\n",
    "pace_list = [1,2,3,4]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of the algorithm with given color histogram difference and true boudary frame list\n",
    "# return recalls and precisions for each parameter set\n",
    "def evaluate_shot_detection(hist_diff, truth):\n",
    "    truth_copy = copy.deepcopy(truth)\n",
    "    measure_indices = []\n",
    "    for sl in shot_length_list:\n",
    "        for sd in shot_diff_list:\n",
    "            for p in pace_list:\n",
    "                # for each set of parameters\n",
    "                if p <= sl: # otherwise we will miss checking some frames\n",
    "                    predicts = find_boundary(hist_diff, shot_len=sl, shot_diff=sd, pace=p)\n",
    "                    predicts_copy = copy.deepcopy(predicts)\n",
    "                    correct = 0\n",
    "                    # if the predict frame is close to the true frame, add 1 to correct\n",
    "                    for predict in predicts:\n",
    "                        if predict in truth:\n",
    "                            correct += 1\n",
    "                            truth_copy.remove(predict)\n",
    "                            predicts_copy.remove(predict)\n",
    "                        elif predict+1 in truth:\n",
    "                            correct += 1\n",
    "                            truth_copy.remove(predict+1)\n",
    "                            predicts_copy.remove(predict)\n",
    "                        elif predict-1 in truth:\n",
    "                            correct += 1\n",
    "                            truth_copy.remove(predict-1)\n",
    "                            predicts_copy.remove(predict)\n",
    "                    # count number of missing ture value and number of errors that are predict as true\n",
    "                    miss = len(truth_copy)\n",
    "                    false_positive = len(predicts_copy)\n",
    "                    # restore global truth for next iteration\n",
    "                    truth_copy = copy.deepcopy(truth)\n",
    "                    # calculate indices\n",
    "                    recall = correct/(correct+miss)\n",
    "                    precision = correct/(correct+false_positive)\n",
    "                    measure_indices.append([recall,precision])\n",
    "    return np.array(measure_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logo detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare how to image different directly\n",
    "# input are list of pictures in numpy array\n",
    "def picture_comp_gray(all_frame, iteration = 20, threshold = 2):\n",
    "    result = np.ones(all_frame[0].shape[0:2], dtype=bool)\n",
    "    while iteration > 0:\n",
    "        rand = random.sample(range(len(all_frame)), 2)\n",
    "        a = cv.cvtColor(all_frame_1[rand[0]], cv.COLOR_BGR2GRAY) \n",
    "        b = cv.cvtColor(all_frame_1[rand[1]], cv.COLOR_BGR2GRAY) \n",
    "        diff = np.absolute(a - b) < threshold\n",
    "        result = np.logical_and(result, diff)\n",
    "        iteration -= 1\n",
    "    return np.transpose(result.nonzero())\n",
    "def picture_comp(all_frame, iteration = 20, threshold = 2):\n",
    "    result = np.ones(all_frame[0].shape[0:2], dtype=bool)\n",
    "    while iteration > 0:\n",
    "        rand = random.sample(range(len(all_frame)), 2)\n",
    "        a = all_frame_1[rand[0]]\n",
    "        b = all_frame_1[rand[1]]\n",
    "        diff = np.absolute(a - b) < threshold\n",
    "        result = np.logical_and(result, diff[:,:,0])\n",
    "        result = np.logical_and(result, diff[:,:,1])\n",
    "        result = np.logical_and(result, diff[:,:,2])\n",
    "        iteration -= 1\n",
    "    return np.transpose(result.nonzero())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force on compare random frames for one picture\n",
    "# works not well\n",
    "def comp_1(all_frame, iteration=30,threshold=10, frame_to_comp = 0):\n",
    "    # extract input picture's feature point\n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "    bf = cv.BFMatcher()\n",
    "    result = []\n",
    "    flag = 0\n",
    "    a =  all_frame[frame_to_comp]\n",
    "    kp_1, des_1 = sift.detectAndCompute(a,None)\n",
    "    while iteration > 0:\n",
    "        # randomly select one picture\n",
    "        rand = random.sample(range(len(all_frame)), 1)\n",
    "        b =  all_frame[rand[0]]\n",
    "        # find the keypoints and descriptors with SIFT\n",
    "        kp_2, des_2 = sift.detectAndCompute(b,None)\n",
    "        matches = bf.knnMatch(des_1,des_2, k=2)\n",
    "\n",
    "        # Apply ratio test\n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.75*n.distance:\n",
    "                good.append(m)\n",
    "        # add feature points that has good match in both pictures\n",
    "        for match in good:\n",
    "            match_1 = kp_1[match.queryIdx]\n",
    "            result.append((int(match_1.pt[0]),int(match_1.pt[1])))\n",
    "        iteration -= 1\n",
    "    # find those feature points in input picture that can find match in other frames\n",
    "    high_occur = []\n",
    "    for pt in result:\n",
    "        if result.count(pt) > 20:\n",
    "            high_occur.append(pt)\n",
    "    # remove duplicate\n",
    "    if len(high_occur) > 0:\n",
    "            high_occur = np.unique(np.array(high_occur), axis=0)\n",
    "    return high_occur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find feature points that can be found in most pictures with fixed position\n",
    "def feature_comp(all_frame, iteration=10,threshold=10):\n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "    bf = cv.BFMatcher()\n",
    "    result = []\n",
    "    while iteration > 0:\n",
    "        # randomly pick 2 images and extract their feature point\n",
    "        rand = random.sample(range(len(all_frame)), 2)\n",
    "        a =  all_frame[rand[0]]\n",
    "        b =  all_frame[rand[1]]\n",
    "        # find the keypoints and descriptors with SIFT\n",
    "        kp_1, des_1 = sift.detectAndCompute(a,None)\n",
    "        kp_2, des_2 = sift.detectAndCompute(b,None)\n",
    "        matches = bf.knnMatch(des_1,des_2, k=2)\n",
    "\n",
    "        # Apply ratio test\n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.75*n.distance:\n",
    "                good.append(m)\n",
    "        # find matching points that in same or close place\n",
    "        same_position = []\n",
    "        for match in good:\n",
    "            match_1 = kp_1[match.queryIdx]\n",
    "            match_2 = kp_2[match.trainIdx]\n",
    "            # if a match feature has same position, add it to same_position\n",
    "            if np.allclose(np.array(match_1.pt, dtype = np.int32), np.array(match_2.pt, dtype = np.int32), atol=threshold):\n",
    "                find_match = np.array(match_1.pt, dtype= np.int32)\n",
    "                if result == []:\n",
    "                    same_position.append(find_match)\n",
    "                else:\n",
    "                    for prev_find in result:\n",
    "                        if np.allclose(prev_find, find_match, atol=threshold):\n",
    "                            same_position.append(prev_find)\n",
    "                            same_position.append(find_match)\n",
    "                            break\n",
    "        # remove duplicate points\n",
    "        result = same_position\n",
    "        if len(result) > 0:\n",
    "            result = np.unique(np.array(same_position), axis=0)\n",
    "        iteration -= 1\n",
    "        \n",
    "    return result       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "def detect_face_helper(image):\n",
    "    face_cascade = cv.CascadeClassifier(\"/home/yige/anaconda3/share/OpenCV/haarcascades/haarcascade_frontalface_alt.xml\")\n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop the image using OpenCV face detector\n",
    "male_dir = \"./project1/train_data/male/\"\n",
    "female_dir = \"./project1/train_data/female/\"\n",
    "male_out = \"./train/male/\"\n",
    "female_out = \"./train/female/\"\n",
    "for i in range(1,261):# image_001 to image_260\n",
    "    index = \"\"\n",
    "    if i < 10:\n",
    "        index = \"00\" + str(i)\n",
    "    elif i< 100:\n",
    "        index = \"0\" + str(i)\n",
    "    else:\n",
    "        index = str(i)\n",
    "    image_male = cv.imread(male_dir + \"image_\" + index + \".jpg\")\n",
    "    faces = detect_face_helper(image_male)\n",
    "    if len(faces) == 1:\n",
    "        RGB_img = cv.cvtColor(image_male, cv.COLOR_BGR2RGB)\n",
    "        bound = max(faces[0][2], faces[0][3])\n",
    "        x,y = faces[0][0], faces[0][1]\n",
    "        male_face = RGB_img[y:y+bound, x:x+bound, :]\n",
    "#         io.imsave(male_out+str(i)+\".jpg\", male_face)\n",
    "    \n",
    "    image_female = cv.imread(female_dir + \"image_\" + index + \".jpg\")\n",
    "    faces = detect_face_helper(image_female)\n",
    "    if len(faces) == 1:\n",
    "        RGB_img = cv.cvtColor(image_female, cv.COLOR_BGR2RGB)\n",
    "        bound = max(faces[0][2], faces[0][3])\n",
    "        x,y = faces[0][0], faces[0][1]\n",
    "        female_face = RGB_img[y:y+bound, x:x+bound, :]\n",
    "#         io.imsave(female_out+str(i)+\".jpg\", female_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our transformation function\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize((227,227)),\n",
    "    # data augmentation, randomly flip picture\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize inputs with mean 0.5, std 0.5\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# training data are manually seperate into differnt folder\n",
    "# directory: train -> train -> -> male/female ->picutres 1.jpg 2.jpg...\n",
    "#                 -> test ->  male/female ->picutres 400.jpg 401.jpg...\n",
    "train_data = datasets.ImageFolder(root='./train/train', transform=train_trans)\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=4)\n",
    "test_data = datasets.ImageFolder(root='./train/test', transform=train_trans)\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model - with respect of AlexNet\n",
    "class simpleAlex(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolution layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=96,kernel_size=11,stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        # fully connect layer\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Linear(in_features=9216, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, train):\n",
    "        output = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(train)))))\n",
    "        # flattern\n",
    "        output = output.view(-1, 9216)\n",
    "        output = self.layer8(self.layer7(self.layer6(output)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleAlex(2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "learning = 0.0001\n",
    "optimizer = Adam(params=model.parameters(),lr=learning)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "# save a model\n",
    "def save_models(epoch):\n",
    "    torch.save(model.state_dict(), \"./models/gender_classifier_{}.model\".format(epoch))\n",
    "    print(\"checkpoint\")\n",
    "\n",
    "# dynamic adjusting learning rate\n",
    "def adjust_lr(epoch, lr=0.learning):\n",
    "    if epoch > 120:\n",
    "        lr = lr/1000000\n",
    "    elif epoch > 100:\n",
    "        lr = lr/100000\n",
    "    elif epoch > 80:\n",
    "        lr = lr/10000\n",
    "    elif epoch > 60:\n",
    "        lr = lr/1000\n",
    "    elif epoch > 40:\n",
    "        lr = lr/100\n",
    "    elif epoch > 20:\n",
    "        lr = lr/10\n",
    "    \n",
    "    for params in optimizer.param_groups:\n",
    "        print(\"learning rate: {}\".format(lr))\n",
    "        params[\"lr\"] = lr\n",
    "\n",
    "# return model accuarcy on test dataset\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "            t_img, t_label = data\n",
    "            t_img = t_img.cuda()\n",
    "            t_label = t_label.cuda()\n",
    "            outputs = model(t_img)\n",
    "            _,predicted = torch.max(outputs.data, 1)\n",
    "            total += t_label.size(0)\n",
    "            correct += (predicted == t_label).sum().item()\n",
    "    return correct/total\n",
    "\n",
    "# return model accuarcy on train dataset\n",
    "def test_train():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in train_loader:\n",
    "            t_img, t_label = data\n",
    "            t_img = t_img.cuda()\n",
    "            t_label = t_label.cuda()\n",
    "            outputs = model(t_img)\n",
    "            _,predicted = torch.max(outputs.data, 1)\n",
    "            total += t_label.size(0)\n",
    "            correct += (predicted == t_label).sum().item()\n",
    "    return correct/total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoches):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        print(\"===================\")\n",
    "        for idx, (images, labels) in enumerate(train_loader, 0):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # reset optimizer's gradients to zero, every .step() will add gradients to model\n",
    "            # reset gradient to avoid duplicate addition\n",
    "            optimizer.zero_grad()\n",
    "            # make prediction\n",
    "            outputs = model(images)\n",
    "            # evaluate loss\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # update model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # add loss number each batch size iteration\n",
    "            running_loss += loss.item()\n",
    "        # calcualte loss rate for this epoch\n",
    "        running_loss /= len(train_data)\n",
    "        \n",
    "        # adjust learning rate if necessary\n",
    "        adjust_lr(epoch)\n",
    "        \n",
    "        # use test dataset to evaluate model\n",
    "        train_acc =test_train()\n",
    "        test_acc = test()\n",
    "        # if it is good enough or improved, save the model\n",
    "        if test_acc > best_acc or test_acc > 0.98:\n",
    "            save_models(epoch, test_acc)\n",
    "            best_acc = test_acc\n",
    "        print(\"epoch {}, test accuarcy {}, training loss {}, train accuarcy {}\".format(epoch, test_acc, running_loss, train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simpleAlex(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=2)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer6): Sequential(\n",
       "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "  )\n",
       "  (layer7): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "  )\n",
       "  (layer8): Linear(in_features=4096, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "checkpoint = torch.load(\"./models/gender_classifier_18_97.model\",map_location=device)\n",
    "simpleA_net = simpleAlex(num_classes=2)\n",
    "simpleA_net.load_state_dict(checkpoint)\n",
    "simpleA_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender(pic):\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(227),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    image_tensor = transformation(pic).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "    output = simpleA_net(image_tensor)\n",
    "    _,predicted = torch.max(output.data, 1)\n",
    "    if int(predicted) == 0:\n",
    "        return 'female'\n",
    "    else:\n",
    "        return 'male'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
